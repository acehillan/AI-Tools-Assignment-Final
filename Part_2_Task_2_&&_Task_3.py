# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZkMjemmxRl_lSlbPnPzozzU0WBWUF1lX
"""

# Cell 1: Imports and Setup
import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
import numpy as np
import matplotlib.pyplot as plt

print(f"TensorFlow version: {tf.__version__}")

# Cell 2: Load and Preprocess Data

# 1. Load Data
(X_train, y_train), (X_test, y_test) = mnist.load_data()

# 2. Normalize Pixel Values (scale to 0-1)
# This is crucial for deep learning models to converge faster and better.
X_train = X_train.astype('float32') / 255.0
X_test = X_test.astype('float32') / 255.0

# 3. Reshape for CNN Input (add channel dimension: (N, 28, 28) -> (N, 28, 28, 1))
# MNIST is grayscale, so the channel depth is 1.
X_train = np.expand_dims(X_train, -1)
X_test = np.expand_dims(X_test, -1)

print(f"X_train shape after preprocessing: {X_train.shape}")
print(f"y_train shape: {y_train.shape}")

# Cell 3: Build, Compile, and Train the CNN

# 1. Build the CNN Model (Sequential API)
model = Sequential([
    # Input Layer + First Conv Block
    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    MaxPooling2D((2, 2)),

    # Second Conv Block
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),

    # Classification Head
    Flatten(), # Converts the 3D feature maps to a 1D vector
    Dense(10, activation='softmax') # Output layer: 10 units for 10 classes (digits 0-9)
])

# 2. Compile Model
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy', # Used because labels (y) are integers (0-9)
    metrics=['accuracy']
)

model.summary() # Print the model structure

# 3. Train Model
print("\nStarting model training...")
history = model.fit(
    X_train, y_train,
    epochs=10, # A small number of epochs is enough for high accuracy on MNIST
    batch_size=32,
    validation_data=(X_test, y_test),
    verbose=2 # Verbose=2 prints one line per epoch
)

# Cell 4: Final Evaluation and Visualization (Key Deliverable)

# 1. Final Evaluation (Check the accuracy requirement: >95%)
loss, acc = model.evaluate(X_test, y_test, verbose=0)
print(f"\n--- Task 2 Final Test Accuracy ---")
print(f"Test Accuracy: {acc:.4f} (Goal: >0.95)")

# 2. Predict on Test Set
predictions = model.predict(X_test)

# 3. Visualize 5 Sample Predictions
fig, axes = plt.subplots(1, 5, figsize=(15, 4))
fig.suptitle('Model Predictions on 5 Test Samples', fontsize=16)

# Generate 5 random indices to sample from the test set
random_indices = np.random.choice(len(X_test), 5, replace=False)

for i, idx in enumerate(random_indices):

    # Get the image and labels
    image = X_test[idx].squeeze() # Remove channel dimension for plotting
    true_label = y_test[idx]
    predicted_label = np.argmax(predictions[idx]) # argmax gives the predicted digit

    axes[i].imshow(image, cmap='gray')
    axes[i].set_title(f"True: {true_label}\nPred: {predicted_label}",
                      color='green' if true_label == predicted_label else 'red')
    axes[i].axis('off')

plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

